# -*- coding: utf-8 -*-
"""3014_Lab9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sTeWnATDkLZop2bX2V_adGCzX3TdNvEl
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt

data = load_breast_cancer(as_frame=True)
X = data.data
y = data.target
feature_names = X.columns.tolist()
target_names = data.target_names.tolist()

X.shape
y.value_counts()
X.isna().sum().sum()

display(X.head(8))

corr = X.corr()
plt.figure(figsize=(8,6))
plt.imshow(corr, interpolation='nearest', aspect='auto')
plt.colorbar()
plt.title("Feature correlation matrix")
plt.xticks(ticks=np.arange(len(feature_names)), labels=feature_names, rotation=90, fontsize=6)
plt.yticks(ticks=np.arange(len(feature_names)), labels=feature_names, fontsize=6)
plt.tight_layout()
plt.show()

top3 = X.var().sort_values(ascending=False).index[:3].tolist()
plt.figure(figsize=(6,4))
plt.scatter(X[top3[0]], X[top3[1]])
plt.xlabel(top3[0]); plt.ylabel(top3[1])
plt.title(f"Scatter: {top3[0]} vs {top3[1]}")
plt.tight_layout()
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)

baseline = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
baseline.fit(X_train, y_train)
y_pred = baseline.predict(X_test)
print("Baseline accuracy:", accuracy_score(y_test, y_pred))
print("Classification report (baseline):\n", classification_report(y_test, y_pred, target_names=target_names))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
plt.imshow(cm, interpolation='nearest')
plt.title("Confusion matrix (baseline)")
plt.colorbar()
plt.xticks([0,1], target_names)
plt.yticks([0,1], target_names)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i,j], ha='center', va='center')
plt.xlabel("Predicted"); plt.ylabel("True")
plt.tight_layout(); plt.show()

importances = baseline.feature_importances_
feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)
print("Top 10 features:\n", feat_imp.head(10))
plt.figure(figsize=(8,4))
plt.bar(range(10), feat_imp.head(10).values)
plt.xticks(range(10), feat_imp.head(10).index, rotation=90)
plt.title("Top 10 Feature Importances (baseline RF)")
plt.tight_layout(); plt.show()

n_list = list(range(10, 401, 30))
oob_errors = []
test_scores = []
for n in n_list:
    rf = RandomForestClassifier(n_estimators=n, oob_score=True, n_jobs=-1, random_state=42)
    rf.fit(X_train, y_train)
    # oob_score_ is available only if oob_score=True and enough trees were used
    oob_errors.append(1 - rf.oob_score_)
    test_scores.append(accuracy_score(y_test, rf.predict(X_test)))

plt.figure(figsize=(8,4))
plt.plot(n_list, oob_errors, marker='o')
plt.title("OOB Error vs n_estimators")
plt.xlabel("n_estimators"); plt.ylabel("OOB error (1 - oob_score_)")
plt.grid(True); plt.tight_layout(); plt.show()

plt.figure(figsize=(8,4))
plt.plot(n_list, test_scores, marker='o')
plt.title("Test Accuracy vs n_estimators")
plt.xlabel("n_estimators"); plt.ylabel("Test accuracy")
plt.grid(True); plt.tight_layout(); plt.show()

# Training accuracy
train_acc = baseline.score(X_train, y_train)
print("Training Accuracy:", train_acc)

# Test accuracy
test_acc = baseline.score(X_test, y_test)
print("Test Accuracy:", test_acc)

param_grid = {
    "n_estimators": [50, 100, 200],
    "max_features": ["sqrt", "log2", 0.5],
    "max_depth": [None, 5, 10, 20],
    "max_leaf_nodes": [None, 10, 30, 100]
}
rf = RandomForestClassifier(random_state=42, n_jobs=-1)
grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid.fit(X_train, y_train)

print("Best params:", grid.best_params_)
print("Best CV score:", grid.best_score_)

best = grid.best_estimator_
y_pred_best = best.predict(X_test)
print("Test accuracy (best):", accuracy_score(y_test, y_pred_best))
print("Classification report (best):\n", classification_report(y_test, y_pred_best, target_names=target_names))

cm_best = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(5,4))
plt.imshow(cm_best, interpolation='nearest')
plt.title("Confusion matrix (best RF)")
plt.colorbar()
plt.xticks([0,1], target_names); plt.yticks([0,1], target_names)
for i in range(cm_best.shape[0]):
    for j in range(cm_best.shape[1]):
        plt.text(j, i, cm_best[i,j], ha='center', va='center')
plt.xlabel("Predicted"); plt.ylabel("True"); plt.tight_layout(); plt.show()

feat_imp_best = pd.Series(best.feature_importances_, index=feature_names).sort_values(ascending=False)
print("Top 10 features (best model):\n", feat_imp_best.head(10))
plt.figure(figsize=(8,4))
plt.bar(range(10), feat_imp_best.head(10).values)
plt.xticks(range(10), feat_imp_best.head(10).index, rotation=90)
plt.title("Top 10 Feature Importances (best RF)")
plt.tight_layout(); plt.show()

print("Training Accuracy (best):", best.score(X_train, y_train))
print("Test Accuracy (best):", best.score(X_test, y_test))

