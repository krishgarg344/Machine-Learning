# -*- coding: utf-8 -*-
"""3014_Lab5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wGX3dj4NKS0ktYo-3Lp1WdRySC2ZIKnP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

df = pd.read_csv('Heart_disease_statlog.csv')

df.info()

df.head()

df.columns

df.shape

df.isnull().sum().sum()

col = list(df.columns)

for i in col:
  if(df[i].dtype == 'int64' or df[i].dtype == 'float64'):
    plt.boxplot(df[i])
    plt.xlabel(i)
    plt.ylabel('Range')
    plt.title(i)
    plt.show()

for i in col:
  q1 = df[i].quantile(0.25)
  q3 = df[i].quantile(0.75)
  iqr = q3-q1
  lf = q1-1.5*iqr
  uf = q3+1.5*iqr
  df = df[(df[i]>=lf) & (df[i]<=uf)]

df.corr()

df.drop(columns=['fbs'], inplace=True)

sns.heatmap(df.corr(),annot=True)

y = df['target']
X = df.drop(columns=['target'])

X.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.model_selection import GridSearchCV

#creating a collection of parameters to be tried
params = {'criterion':['gini','entropy'],'max_depth':[3,4,5,6,7,8]}
#initiating the Decision Tree Classifier
dec_tree = DecisionTreeClassifier()
#GridSearchCV takes the classifier and list of hyperparameters to be tried to on the classifier
grid = GridSearchCV(dec_tree, param_grid=params, cv=5)
#Applying the grid on the training data and it gives the best combination out of all
grid_model = grid.fit(X_train, y_train)

grid.best_params_

grid.best_score_

grid.best_estimator_

clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

from sklearn import tree
# plotting decision tree
fig = plt.figure(figsize=(20, 10))
fig = tree.plot_tree(clf, feature_names = X.columns, filled = True)

# getting importance
importance = clf.feature_importances_
# plotting feature importance
for i, v in enumerate(importance):
  print(f'Feature: {i}, Score: {v:.5f}')
plt.bar([x for x in range(len(importance))], importance)
plt.show()

clf = DecisionTreeClassifier(random_state=0)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
ccp_alphas

fig, ax = plt.subplots()
#the maximum effective alpha value is removed, because it is the trivial tree with only one node.
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha1 in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha1)
    clf.fit(X_train, y_train)
    clfs.append(clf)
    print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alpha1
    )
)

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker="o", label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()

